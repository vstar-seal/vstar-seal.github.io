@inproceedings{sundararajan2017axiomatic,
    title={Axiomatic attribution for deep networks},
    author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
    booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
    pages={3319--3328},
    year={2017},
    organization={JMLR. org},
    url={https://arxiv.org/pdf/1703.01365}
}

@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017},
  url={https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14806/14311}
}
 
@inproceedings{deng2009imagenet,
    title={Imagenet: A large-scale hierarchical image database},
    author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
    booktitle={2009 IEEE conference on computer vision and pattern recognition},
    pages={248--255},
    year={2009},
    organization={Ieee},
    url={https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015},
  url={https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf}
}

@article{silberman2016tensorflow,
  title={Tensorflow-slim image classification model library},
  author={Silberman, Nathan and Guadarrama, Sergio},
  year={2016},
  publisher={Accessed},
  url={https://github.com/tensorflow/models/tree/master/research/slim}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3145--3153},
  year={2017},
  organization={JMLR. org},
  url={https://arxiv.org/pdf/1704.02685}
}

@inproceedings{binder2016layer,
  title={Layer-wise relevance propagation for neural networks with local renormalization layers},
  author={Binder, Alexander and Montavon, Gregoire and Lapuschkin, Sebastian and Muller, Klaus-Robert and Samek, Wojciech},
  booktitle={International Conference on Artificial Neural Networks},
  pages={63--71},
  year={2016},
  organization={Springer},
  url={https://arxiv.org/pdf/1604.00825.pdf}
}

@article{smilkov2017smoothgrad,
  title={Smoothgrad: removing noise by adding noise},
  author={Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viegas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1706.03825},
  year={2017},
  url={https://arxiv.org/pdf/1706.03825.pdf}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4765--4774},
  year={2017},
  url={http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}
}

@book{aumann2015values,
  title={Values of non-atomic games},
  author={Aumann, Robert J and Shapley, Lloyd S},
  year={2015},
  publisher={Princeton University Press},
  url={https://www.jstor.org/stable/j.ctt13x149m}
}

@article{friedman2004paths,
  title={Paths and consistency in additive cost sharing},
  author={Friedman, Eric J},
  journal={International Journal of Game Theory},
  volume={32},
  number={4},
  pages={501--518},
  year={2004},
  publisher={Springer},
  url={https://link.springer.com/content/pdf/10.1007/s001820400173.pdf}
}

@misc{erion2019learning,
    title={Learning Explainable Models Using Attribution Priors},
    author={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott Lundberg and Su-In Lee},
    year={2019},
    eprint={1906.10670},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/pdf/1906.10670.pdf}
}

@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@article{kim2017interpretability,
  title={Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)},
  author={Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  journal={arXiv preprint arXiv:1711.11279},
  year={2017}
}

@inproceedings{fong2017interpretable,
  title={Interpretable explanations of black boxes by meaningful perturbation},
  author={Fong, Ruth C and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3429--3437},
  year={2017}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={618--626},
  year={2017}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@misc{simonyan2013deep,
    title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
    author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
    year={2013},
    eprint={1312.6034},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{fong2018net2vec,
  title={Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks},
  author={Fong, Ruth and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8730--8738},
  year={2018}
}

@misc{ribeiro2016i,
    title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
    author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
    year={2016},
    eprint={1602.04938},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{zhou2016learning,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}

@article{erhan2009visualizing,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={University of Montreal},
  volume={1341},
  number={3},
  pages={1},
  year={2009}
}

@inproceedings{mahendran2015understanding,
  title={Understanding deep image representations by inverting them},
  author={Mahendran, Aravindh and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5188--5196},
  year={2015}
}

@article{sayres2019using,
  title={Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy},
  author={Sayres, Rory and Taly, Ankur and Rahimy, Ehsan and Blumer, Katy and Coz, David and Hammel, Naama and Krause, Jonathan and Narayanaswamy, Arunachalam and Rastegar, Zahra and Wu, Derek and others},
  journal={Ophthalmology},
  volume={126},
  number={4},
  pages={552--564},
  year={2019},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0161642018315756}
}
@article{warrick2018ensembling,
  title={Ensembling convolutional and long short-term memory networks for electrocardiogram arrhythmia detection},
  author={Warrick, Philip A and Homsi, Masun Nabhan},
  journal={Physiological measurement},
  volume={39},
  number={11},
  pages={114002},
  year={2018},
  publisher={IOP Publishing},
  url={https://iopscience.iop.org/article/10.1088/1361-6579/aad386/meta}
}

@article{sundararajan2016gradients,
  title={Gradients of counterfactuals},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  journal={arXiv preprint arXiv:1611.02639},
  year={2016},
  url={https://arxiv.org/pdf/1611.02639.pdf}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6389--6399},
  year={2018},
  url={http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  url={http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}
}

@misc{kindermans2017unreliability,
    title={The (Un)reliability of saliency methods},
    author={Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
    year={2017},
    eprint={1711.00867},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{ancona2017better,
    title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},
    author={Marco Ancona and Enea Ceolini and Cengiz Öztireli and Markus Gross},
    year={2017},
    eprint={1711.06104},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{sundararajan2018note,
  title={A note about: Local explanation methods for deep neural networks lack sensitivity to parameter values},
  author={Sundararajan, Mukund and Taly, Ankur},
  journal={arXiv preprint arXiv:1806.04205},
  year={2018}
}

@misc{kapishnikov2019xrai,
    title={XRAI: Better Attributions Through Regions},
    author={Andrei Kapishnikov and Tolga Bolukbasi and Fernanda Viégas and Michael Terry},
    year={2019},
    eprint={1906.02825},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{sundararajan2019shapley,
    title={The many Shapley values for model explanation},
    author={Mukund Sundararajan and Amir Najmi},
    year={2019},
    eprint={1908.08474},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{janzing2019feature,
    title={Feature relevance quantification in explainable AI: A causality problem},
    author={Dominik Janzing and Lenon Minorics and Patrick Blöbaum},
    year={2019},
    eprint={1910.13413},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{ross2017right,
  title={Right for the right reasons: Training differentiable models by constraining their explanations},
  author={Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1703.03717},
  year={2017}
}

@misc{hooker2018benchmark,
    title={A Benchmark for Interpretability Methods in Deep Neural Networks},
    author={Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
    year={2018},
    eprint={1806.10758},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{yeh2019infidelity,
    title={On the (In)fidelity and Sensitivity for Explanations},
    author={Chih-Kuan Yeh and Cheng-Yu Hsieh and Arun Sai Suggala and David I. Inouye and Pradeep Ravikumar},
    year={2019},
    eprint={1901.09392},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{kindermans2017unreliability,
    title={The (Un)reliability of saliency methods},
    author={Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
    year={2017},
    eprint={1711.00867},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{adebayo2018sanity,
    title={Sanity Checks for Saliency Maps},
    author={Julius Adebayo and Justin Gilmer and Michael Muelly and Ian Goodfellow and Moritz Hardt and Been Kim},
    year={2018},
    eprint={1810.03292},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{yang2019benchmarking,
    title={Benchmarking Attribution Methods with Relative Feature Importance},
    author={Mengjiao Yang and Been Kim},
    year={2019},
    eprint={1907.09701},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lin2019explanations,
    title={Do Explanations Reflect Decisions? A Machine-centric Strategy to Quantify the Performance of Explainability Algorithms},
    author={Zhong Qiu Lin and Mohammad Javad Shafiee and Stanislav Bochkarev and Michael St. Jules and Xiao Yu Wang and Alexander Wong},
    year={2019},
    eprint={1910.07387},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{narayanan2018humans,
    title={How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation},
    author={Menaka Narayanan and Emily Chen and Jeffrey He and Been Kim and Sam Gershman and Finale Doshi-Velez},
    year={2018},
    eprint={1802.00682},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{ghorbani2019interpretation,
  title={Interpretation of neural networks is fragile},
  author={Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={3681--3688},
  year={2019}
}

@inproceedings{anne2018grounding,
  title={Grounding visual explanations},
  author={Anne Hendricks, Lisa and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={264--279},
  year={2018}
}

@inproceedings{hendricks2016generating,
title={Generating visual explanations},
author={Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
booktitle={European Conference on Computer Vision},
pages={3--19},
year={2016},
organization={Springer}
}
  
 @article{petsiuk2018rise,
   title={Rise: Randomized input sampling for explanation of black-box models},
   author={Petsiuk, Vitali and Das, Abir and Saenko, Kate},
   journal={arXiv preprint arXiv:1806.07421},
   year={2018}
 }
 
@article{chen2019explaining,
  title={Explaining Models by Propagating Shapley Values of Local Components},
  author={Chen, Hugh and Lundberg, Scott and Lee, Su-In},
  journal={arXiv preprint arXiv:1911.11888},
  year={2019}
}

@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{gpt4roi,
  title={Gpt4roi: Instruction tuning large language model on region-of-interest},
  author={Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
  journal={arXiv preprint arXiv:2307.03601},
  year={2023}
}



@article{mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@article{mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{lvlm-ehub,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@article{seed-bench,
  title={SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}

@article{wang2023makes,
  title={What Makes for Good Visual Tokenizers for Large Language Models?},
  author={Wang, Guangzhi and Ge, Yixiao and Ding, Xiaohan and Kankanhalli, Mohan and Shan, Ying},
  journal={arXiv preprint arXiv:2305.12223},
  year={2023}
}

@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@article{groundingdino,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}

@article{LISA,
  title={LISA: Reasoning Segmentation via Large Language Model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2308.00692},
  year={2023}
}

@article{contextDET,
  title={Contextual Object Detection with Multimodal Large Language Models},
  author={Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change},
  journal={arXiv preprint arXiv:2305.18279},
  year={2023}
}

@inproceedings{owl-vit,
  title={Simple open-vocabulary object detection},
  author={Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and others},
  booktitle={ECCV},
  year={2022},
}

@inproceedings{COCO,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014},
}

@inproceedings{GQA,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@InProceedings{vaw,
    author    = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
    title     = {Learning To Predict Visual Attributes in the Wild},
    booktitle = {CVPR},
    year      = {2021},
}

@article{SAM,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}

@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{minigpt4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{InstructBLIP,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi},
  journal={arXiv preprint arXiv:2305.06500},
  year={2023},
}

@article{blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@inproceedings{flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  booktitle={NeurIPS},
  year={2022}
}

@article{otter,
  title={Mimic-it: Multi-modal in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@article{mm-react,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@inproceedings{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021},
}

@article{ChatCaptioner,
  title={Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
  author={Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2303.06594},
  year={2023}
}

@article{vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2303.08128},
  year={2023}
}

@article{chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.09842},
  year={2023}
}


@article{idealgpt,
  title={IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models},
  author={You, Haoxuan and Sun, Rui and Wang, Zhecan and Chen, Long and Wang, Gengyu and Ayyubi, Hammad A and Chang, Kai-Wei and Chang, Shih-Fu},
  journal={arXiv preprint arXiv:2305.14985},
  year={2023}
}

@article{AVIS,
  title={AVIS: Autonomous Visual Information Seeking with Large Language Models},
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Chang, Kai-Wei and Sun, Yizhou and Ross, David A and Schmid, Cordelia and Fathi, Alireza},
  journal={arXiv preprint arXiv:2306.08129},
  year={2023}
}

@inproceedings{openseed,
  title={A simple framework for open-vocabulary segmentation and detection},
  author={Zhang, Hao and Li, Feng and Zou, Xueyan and Liu, Shilong and Li, Chunyuan and Yang, Jianwei and Zhang, Lei},
  booktitle={ICCV},
  year={2023}
}

@article{ViLD,
  title={Open-vocabulary object detection via vision and language knowledge distillation},
  author={Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
  journal={arXiv preprint arXiv:2104.13921},
  year={2021}
}

@inproceedings{GLIP,
  title={Grounded language-image pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{OV-DETR,
  title={Open-vocabulary detr with conditional matching},
  author={Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
  booktitle={ECCV},
  year={2022},
}

@article{VLDet,
  title={Learning object-language alignments for open-vocabulary object detection},
  author={Lin, Chuang and Sun, Peize and Jiang, Yi and Luo, Ping and Qu, Lizhen and Haffari, Gholamreza and Yuan, Zehuan and Cai, Jianfei},
  journal={arXiv preprint arXiv:2211.14843},
  year={2022}
}

@inproceedings{luo2020multi,
  title={Multi-task collaborative network for joint referring expression comprehension and segmentation},
  author={Luo, Gen and Zhou, Yiyi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Chenglin and Deng, Cheng and Ji, Rongrong},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{li2021referring,
  title={Referring transformer: A one-step approach to multi-task visual grounding},
  author={Li, Muchen and Sigal, Leonid},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Transvg,
  title={Transvg: End-to-end visual grounding with transformers},
  author={Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{reclip,
title = {ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension},
author = {Subramanian, Sanjay  and
Merrill, William  and
Darrell, Trevor  and
Gardner, Matt  and
Singh, Sameer  and
Rohrbach, Anna},
booktitle = {ACL},
year = "2022",
}
@article{xie2023exposing,
  title={Exposing the Troublemakers in Described Object Detection},
  author={Xie, Chi and Zhang, Zhao and Wu, Yixuan and Zhu, Feng and Zhao, Rui and Liang, Shuang},
  journal={arXiv preprint arXiv:2307.12813},
  year={2023}
}

@article{wolfe2011visual,
  title={Visual search in scenes involves selective and nonselective pathways},
  author={Wolfe, Jeremy M and V{\~o}, Melissa L-H and Evans, Karla K and Greene, Michelle R},
  journal={Trends in cognitive sciences},
  volume={15},
  number={2},
  pages={77--84},
  year={2011},
  publisher={Elsevier}
}

@article{torralba2006contextual,
  title={Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.},
  author={Torralba, Antonio and Oliva, Aude and Castelhano, Monica S and Henderson, John M},
  journal={Psychological review},
  volume={113},
  number={4},
  pages={766},
  year={2006},
  publisher={American Psychological Association}
}

@article{wolfe2017five,
  title={Five factors that guide attention in visual search},
  author={Wolfe, Jeremy M and Horowitz, Todd S},
  journal={Nature Human Behaviour},
  volume={1},
  number={3},
  pages={0058},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@article{peelen2011neural,
  title={A neural basis for real-world visual search in human occipitotemporal cortex},
  author={Peelen, Marius V and Kastner, Sabine},
  journal={Proceedings of the National Academy of Sciences},
  volume={108},
  number={29},
  pages={12125--12130},
  year={2011},
  publisher={National Acad Sciences}
}

@article{wang2023statistical,
  title={Statistical learning speeds visual search: More efficient selection, or faster response?},
  author={Wang, Sisi and Cong, Stanislas Huynh and Woodman, Geoffrey F},
  journal={Journal of Experimental Psychology: General},
  year={2023},
  publisher={American Psychological Association}
}

@article{wolfe2020visual,
  title={Visual search: How do we find what we are looking for?},
  author={Wolfe, Jeremy M},
  journal={Annual review of vision science},
  volume={6},
  pages={539--562},
  year={2020},
  publisher={Annual Reviews}
}

@article{sclar2020modeling,
  title={Modeling human visual search: A combined Bayesian searcher and saliency map approach for eye movement guidance in natural scenes},
  author={Sclar, Melanie and Bujia, Gast{\'o}n and Vita, Sebasti{\'a}n and Solovey, Guillermo and Kamienkowski, Juan Esteban},
  journal={arXiv preprint arXiv:2009.08373},
  year={2020}
}
@article{zhang2018finding,
  title={Finding any Waldo with zero-shot invariant and efficient visual search},
  author={Zhang, Mengmi and Feng, Jiashi and Ma, Keng Teck and Lim, Joo Hwee and Zhao, Qi and Kreiman, Gabriel},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={3730},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{yang2020predicting,
  title={Predicting goal-directed human attention using inverse reinforcement learning},
  author={Yang, Zhibo and Huang, Lihan and Chen, Yupei and Wei, Zijun and Ahn, Seoyoung and Zelinsky, Gregory and Samaras, Dimitris and Hoai, Minh},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={193--202},
  year={2020}
}

@inproceedings{mdetr,
  title={Mdetr-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1780--1790},
  year={2021}
}

@inproceedings{objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8430--8439},
  year={2019}
}

@article{llava1.5,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@InProceedings{textvqa,
author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
title = {Towards VQA Models That Can Read},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{SR-RAW,
  title={Zoom to learn, learn to zoom},
  author={Zhang, Xuaner and Chen, Qifeng and Ng, Ren and Koltun, Vladlen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3762--3770},
  year={2019}
}

@article{visualChatGPT,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@inproceedings{ozge2019power,
  title={The power of tiling for small object detection},
  author={Ozge Unel, F and Ozkalayci, Burak O and Cigla, Cevahir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={0--0},
  year={2019}
}

@inproceedings{chen2022scaling,
  title={Scaling vision transformers to gigapixel images via hierarchical self-supervised learning},
  author={Chen, Richard J and Chen, Chengkuan and Li, Yicong and Chen, Tiffany Y and Trister, Andrew D and Krishnan, Rahul G and Mahmood, Faisal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16144--16155},
  year={2022}
}

@inproceedings{cocostuff,
  title={Coco-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1209--1218},
  year={2018}
}

@inproceedings{paco,
  title={Paco: Parts and attributes of common objects},
  author={Ramanathan, Vignesh and Kalia, Anmol and Petrovic, Vladan and Wen, Yi and Zheng, Baixue and Guo, Baishan and Wang, Rui and Marquez, Aaron and Kovvuri, Rama and Kadian, Abhishek and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7141--7151},
  year={2023}
}

@inproceedings{refcoco,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}

@inproceedings{refcocog,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={11--20},
  year={2016}
}

@inproceedings{visprog,
  title={Visual programming: Compositional visual reasoning without training},
  author={Gupta, Tanmay and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14953--14962},
  year={2023}
}

@article{cocosearch18,
  title={COCO-Search18 fixation dataset for predicting goal-directed attention control},
  author={Chen, Yupei and Yang, Zhibo and Ahn, Seoyoung and Samaras, Dimitris and Hoai, Minh and Zelinsky, Gregory},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={1--11},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{POPE,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao and Ji-Rong Wen},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://openreview.net/forum?id=xozJw0kZXF}
}

@article{MM-Vet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@misc{bard,
    title = {Bard},
    url = {https://bard.google.com},
    author = {Google},
    month = {Febraury},
    year = {2023}
}

@article{gpt_4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}

@misc{vicuna,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{DETR,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}

@misc{weatheringwithyou2019,
  title = {Weathering with You},
  author = {Shinkai, Makoto},
  howpublished = {Motion picture},
  year = {2019},
  note = {Produced by CoMix Wave Films. Distributed by Toho},
  url = {https://www.imdb.com/title/tt9426210/}
}

@misc{childrenwhochaselostvoices2011,
  title = {Children Who Chase Lost Voices},
  author = {Shinkai, Makoto},
  howpublished = {Motion picture},
  year = {2011},
  note = {Produced by CoMix Wave Films. Distributed by Sentai Filmworks},
  url = {https://www.imdb.com/title/tt1839494/}
}